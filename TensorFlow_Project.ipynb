{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# TensorFlow - Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### The objective is to build models to classify a series of images into one of ten classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "train_images = np.load(gzip.open('train_images.npy.gz', 'rb'))\n",
    "train_labels = np.load(gzip.open('train_labels.npy.gz', 'rb'))\n",
    "validation_images = np.load(gzip.open('validation_images.npy.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc5097b0a60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.rcParams[\"axes.grid\"] = False  #  Remove the grid lines from the image.\n",
    "matplotlib.pyplot.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9, ..., 9, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names[train_labels[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### First, I'll measure the difference between two images, and choose the label corresponding to nearby images. I make the simplifying assumption that this is just the average difference between the colors of the corresponding pixels in the two images.\n",
    "\n",
    "$$\\Delta C \\equiv \\sqrt{2 \\Delta R^2 + 4 \\Delta G^2 + 3 \\Delta B^2 + \\bar R\\left(\\Delta R^2 - \\Delta B^2 \\right)} $$\n",
    "where $(R_1, G_1, B_1)$ and $(R_2, G_2, B_2)$ are the RGB components of the two colors and\n",
    "$$\\begin{align}\n",
    "\\Delta R &= R_1 - R_2 \\\\\n",
    "\\Delta G &= G_1 - G_2 \\\\\n",
    "\\Delta B &= B_1 - B_2 \\\\\n",
    "\\bar R &= \\textstyle\\frac{1}{2}\\left(R_1 + R_2\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "### This accounts for the fact that our eyes are most sensitive to green and least sensitive to red, and that perception is not constant with hue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def delta_func(images, base): \n",
    "\n",
    "    def color_diff(x,y):\n",
    "                r_diff = tf.square(tf.subtract(x[:,:,0], y[:,:,0]))\n",
    "                g_diff = tf.square(tf.subtract(x[:,:,1], y[:,:,1]))\n",
    "                b_diff = tf.square(tf.subtract(x[:,:,2], y[:,:,2]))\n",
    "                C_d = tf.sqrt(2*r_diff + 4*g_diff + 3*b_diff + tf.divide(tf.add(x[:,:,0], y[:,:,0]),2) * (r_diff - b_diff))\n",
    "                return tf.reduce_sum(C_d / (32*32)) \n",
    "    \n",
    "    return tf.map_fn(lambda i: color_diff(i, base), images)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([0.        , 0.80592716, 1.1263453 , 0.46740696, 0.8712157 ,\n",
       "       0.70549786, 0.77343273, 0.8241726 , 1.1291139 , 0.9068342 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_func(train_images[:10], train_images[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### As stated, for two images, $I_1$ and $I_2$, I define the distance between $I_1$ and $I_2$ as the average $\\Delta C$ value over the whole image, that is:\n",
    "\n",
    "$$d(I_1, I_2) = \\frac{1}{N}\\sum_{p_j} \\Delta C(p_j)$$\n",
    "where the sum is over all pixels $p_j$, $\\Delta C(p_2)$ is the $\\Delta C$ value for the pixel $p_j$, and $N$ is the total number of pixels in each image ($N= 32\\times 32$ in our case).\n",
    "\n",
    "#### Using `delta_func` I compute the distance between the first validation image and all of the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "deltas = delta_func(train_images, validation_images[0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(deltas.argsort()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Now, I'll let neural networks figure out how to connect pixel values to classes starting with something that barely qualifies as a neural network: a softmax classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical as one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_hot = one_hot(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(32,32,3)),\n",
    "        keras.layers.Dense(10, activation='softmax')   \n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels_hot, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = [np.argmax(i) for i in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Next, I add a hidden layer to this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(32,32,3)),\n",
    "        keras.layers.Dense(hidden_size, \n",
    "                           activation='sigmoid',\n",
    "                           kernel_initializer=keras.initializers.TruncatedNormal(stddev=hidden_size**-0.5)\n",
    "                          )\n",
    "                            \n",
    "    ])\n",
    "\n",
    "model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels_hot, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_images)\n",
    "predicted_classes = [np.argmax(i) for i in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### I next build a neural network with convolutional layers to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "train_labels_hot = one_hot(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_images, train_labels_hot, test_size = 0.05, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "n_classes = 10\n",
    "n_channels = 1\n",
    "filt_size = [5, 5] # 5x5 pixel filters\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Reshape([img_size, img_size, 3]))\n",
    "model.add(keras.layers.Conv2D(16, filt_size, padding='same',\n",
    "                              activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2),\n",
    "                                    padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dropout(.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(100, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(n_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_images)\n",
    "predicted_classes = [np.argmax(i) for i in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Finally, I use a network trained on one data set to provide a starting point: the Inception network used in the Deep Dream notebook hy Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# include_top=False will discard avg_pool before prediction layer\n",
    "inception = tf.keras.applications.inception_v3.InceptionV3(include_top=True, input_shape=(299, 299, 3))\n",
    "inception = tf.keras.Model([inception.input], [inception.layers[-2].output]) # manually discard prediction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [inception(tf.image.resize(train_images[i*50:(i+1)*50], (299,299))) for i in range(0, 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectors.npy\", \"wb\") as f:\n",
    "    np.save(f, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vectors = np.load(open(\"vectors.npy\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.concatenate(latent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2048)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "train_labels_hot = one_hot(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(latent, train_labels_hot, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = 299 * 299 * 3\n",
    "n_classes = 10\n",
    "n_channels = 1\n",
    "filt_size = [5, 5] # 5x5 pixel filters\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "hidden_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(\n",
    "    hidden_size,\n",
    "    activation = 'sigmoid',\n",
    "    kernel_initializer=keras.initializers.TruncatedNormal(stddev=pixels**-0.5)\n",
    "    )\n",
    ")\n",
    "          \n",
    "model.add(keras.layers.Dense(\n",
    "    10,\n",
    "    activation='softmax',\n",
    "    kernel_initializer=keras.initializers.TruncatedNormal(stddev=hidden_size**-0.5)\n",
    "    )\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "352/352 [==============================] - 1s 2ms/step - loss: 0.8420 - accuracy: 0.7699 - val_loss: 0.5519 - val_accuracy: 0.8166\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 1s 2ms/step - loss: 0.5033 - accuracy: 0.8331 - val_loss: 0.4667 - val_accuracy: 0.8374\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 1s 2ms/step - loss: 0.4434 - accuracy: 0.8488 - val_loss: 0.4404 - val_accuracy: 0.8436\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 1s 2ms/step - loss: 0.4069 - accuracy: 0.8612 - val_loss: 0.4322 - val_accuracy: 0.8438\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 1s 2ms/step - loss: 0.3791 - accuracy: 0.8717 - val_loss: 0.4346 - val_accuracy: 0.8474\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                   epochs=5,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2 = [inception(tf.image.resize(validation_images[i*50:(i+1)*50], (299,299))) for i in range(0, 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectors2.npy\", \"wb\") as f:\n",
    "    np.save(f, lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vectors = np.load(open(\"vectors2.npy\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = np.concatenate(val_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2048)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation)\n",
    "predicted_classes = [np.argmax(i) for i in y_pred]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
